---
title: "SPOOKY data experiments"
author: "Jerome Kafrouni"
date: "January 31, 2018"
output:
  html_document: default
  pdf_document: default
---

# Introduction

## Setup the libraries

```{r, message = F, warning = F}
packages.used <- c("ggplot2", "dplyr", "tibble", "tidyr",  "stringr", "tidytext", "topicmodels", "wordcloud", "ggridges")

# check packages that need to be installed.
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))

# install additional packages
if(length(packages.needed) > 0) {
  install.packages(packages.needed, dependencies = TRUE, repos = 'http://cran.us.r-project.org')
}

library(ggplot2)
library(dplyr)
library(tibble)
library(tidyr)
library(stringr)
library(tidytext)
library(topicmodels)
library(wordcloud)
library(ggridges)

source("../lib/multiplot.R")

# install.packages("openNLP")
# install.packages("openNLPmodels.en", repos="http://datacube.wu.ac.at/" , type="source")
# install.packages("NLP")
library(rJava)
library(NLP)
library(openNLP)
library(openNLPmodels.en)

# NLP and ggplot2 both have a function named "annotate" which can cause a problem
# loading NLP after ggplot seems to fix the pb ??
```


## Word Frequency

Now we study some of the most common words in the entire data set.  With the below code we plot the fifty most common words in the entire datset. We see that "time", "life", and "night" all appear frequently.

```{r}
# Words is a list of words, and freqs their frequencies
words <- count(group_by(spooky_wrd, word))$word
freqs <- count(group_by(spooky_wrd, word))$n

head(sort(freqs, decreasing = TRUE))
# wordcloud(words, freqs, max.words = 50, color = c("purple4", "red4", "black"))
```


```{r}
# Counts number of times each author used each word.
author_words <- count(group_by(spooky_wrd, word, author))

# Counts number of times each word was used.
all_words    <- rename(count(group_by(spooky_wrd, word)), all = n)

author_words <- left_join(author_words, all_words, by = "word")
author_words <- arrange(author_words, desc(all))
author_words <- ungroup(head(author_words, 81))
  
ggplot(author_words) +
  geom_col(aes(reorder(word, all, FUN = min), n, fill = author)) +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~ author) +
  theme(legend.position = "none")
```

# Writing style

## Dialogue

Pb: sometimes weird single " are in the text ??
=> TODO
```{r}
spooky <- cbind(spooky, dialogue = mapply(function(x) str_count(x, '\\"') > 1, spooky$text))
spooky[spooky$dialogue == TRUE,]
# pb: some sentences have a " in the beginning but not at the end ??
aggregate(dialogue ~ author, spooky, sum)
# ATTENTION A NORMALISER LES SOMMES !
```


## Entities

```{r}
## Example:
# s <- "Pierre Vinken, and Jerome Kafrounisu 61 years old, will join the board"
# s <- as.String(s) # nÃ©cessite library(NLP) ?
# sent_token_annotator <- Maxent_Sent_Token_Annotator()
# word_token_annotator <- Maxent_Word_Token_Annotator()
# a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))

## Need sentence and word token annotations.
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
entity_annotator <- Maxent_Entity_Annotator(kind="person")

# annotate(s, entity_annotator, a2)

contains_entity <- function(s) {
  a2 <- annotate(s, list(sent_token_annotator, word_token_annotator)) # needed ?
  a3 <- annotate(s, entity_annotator, a2)
  length(subset(a3, type == "entity")) > 0
}

spooky <- cbind(spooky, has_entity = mapply(contains_entity, spooky$text))
aggregate(has_entity ~ author, spooky, mean) # HPL and MWS use more entities

```

## Full POS:

### Split into tags

```{r}
pos_tag_annotator <- Maxent_POS_Tag_Annotator()

posify <- function(s) {
  # given a string, replaces each word by corresponding POS tag
  a1 <- annotate(s, list(sent_token_annotator, word_token_annotator))
  a2 <- annotate(s, pos_tag_annotator, a1)
  tags <- sapply(a2$features, `[[`, "POS")
  do.call(paste, c(as.list(tags), sep=" "))
}

spooky <- cbind(spooky, pos = mapply(posify, spooky$text), stringsAsFactors=FALSE)
spooky_pos <- unnest_tokens(spooky, tag, pos)
```

### Tags frequency:

```{r}
# Counts number of times each author used each word.
author_pos <- count(group_by(spooky_pos, tag, author))

all_pos <- rename(count(group_by(spooky_pos, tag)), all = n)
author_pos <- left_join(author_pos, all_pos, by = "tag")
author_pos <- arrange(author_pos, desc(all))
author_pos <- ungroup(head(author_pos, 81))

ggplot(author_pos) +
  geom_col(aes(reorder(tag, all, FUN = min), n, fill = author)) +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~ author) +
  theme(legend.position = "none")
```


# Diversity of the vocabulary:

## Size of author's vocabulary

How many different (unique) words does each author use ?
```{r}
EAP_words_count <- spooky_wrd[spooky_wrd$author == 'EAP',] %>% group_by(word) %>% summarise(count = n())
HPL_words_count <- spooky_wrd[spooky_wrd$author == 'HPL',] %>% group_by(word) %>% summarise(count = n())
MWS_words_count <- spooky_wrd[spooky_wrd$author == 'MWS',] %>% group_by(word) %>% summarise(count = n())

# nb of unique words / total nb of words used: 
dim(EAP_words_count)[1] / dim(spooky_wrd[spooky_wrd$author == 'EAP',])[1]
dim(HPL_words_count)[1] / dim(spooky_wrd[spooky_wrd$author == 'HPL',])[1]
dim(MWS_words_count)[1] / dim(spooky_wrd[spooky_wrd$author == 'MWS',])[1]

 # how many times is a word used on average
mean(EAP_words_count$count)
mean(HPL_words_count$count)
mean(MWS_words_count$count)
```

## Made up words

Pbs of this technique: 1) OCR 2) age of the dictionnary

```{r}
is.word  <- function(x) x %in% GradyAugmented # or use any dataset from package

author_words <- count(group_by(spooky_wrd, word, author))
author_words$exists <- is.word(author_words$word)
author_words %>% group_by(author) %>% summarise(mean(exists)) # percentage of words per author that are in dictionnary
```

## Words used by only one author:

Idea: look at words (lemmas actually) that one author uses and the others don't. Look at the best TF-IDF of these words. This will show which words are deeply correlated with an author and part of their writing style.

```{r}
unique_wrd_EAP <- anti_join(spooky_wrd, spooky_wrd[spooky_wrd$author == 'EAP',], spooky_wrd[spooky_wrd$author != 'EAP',], by = "lemma")
length(unique(unique_wrd_EAP[, c("lemma")])) # 7078

unique_wrd_MWS <- anti_join(spooky_wrd, spooky_wrd[spooky_wrd$author == 'MWS',], spooky_wrd[spooky_wrd$author != 'MWS',], by = "lemma")
length(unique(unique_wrd_MWS[, c("lemma")])) # 10276

unique_wrd_HPL <- anti_join(spooky_wrd, spooky_wrd[spooky_wrd$author == 'HPL',], spooky_wrd[spooky_wrd$author != 'HPL',], by = "lemma")
length(unique(unique_wrd_HPL[, c("lemma")])) # 7493
```


# Male vs female

The goal is to find whether some author speak more about men or women, if they make them do or say particular things, if women speak more about emotions etc.

## Her vs She:

```{r}
## spooky_wrd_with_stopwords <- unnest_tokens(spooky, word, text)
author_words_with_stopwords <- count(group_by(spooky_wrd_with_stopwords, word, author))
author_words_with_stopwords[author_words_with_stopwords$word %in% c('he', 'she', 'him', 'her', 'his'),]
# TODO: plot

ggplot(author_words_with_stopwords[author_words_with_stopwords$word %in% c('he', 'she', 'him', 'her', 'his'),], aes(x = word, fill = author)) +
      geom_histogram(binwidth=1)
# TODO: add male and female names
```

## Bi-grams male vs female

Doing this with or without lemmatization doesn't change the results too much (because anyway after a pronoun the word is likely to be a conjugated verb, most of the time in past tense).

```{r}
# https://juliasilge.com/blog/gender-pronouns/
  
## spooky_bigrams <- unnest_tokens(spooky, bigram, text, token = "ngrams", n = 2)

# TODO: do this with lemmatazation first

pronouns <- c("he", "she")

bigram_counts <- group_by(spooky_bigrams, bigram, author) %>%
    count(bigram, sort = TRUE) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(word1 %in% pronouns) %>%
    count(word1, word2, wt = n, sort = TRUE) %>%
    rename(total = nn)

bigram_counts

word_ratios <- bigram_counts %>%
    group_by(word2, author) %>%
    filter(sum(total) > 10) %>%
    ungroup() %>%
    spread(word1, total, fill = 0) %>%
    mutate_if(is.numeric, funs((. + 1) / sum(. + 1))) %>%
    mutate(logratio = log2(she / he)) %>%
    arrange(desc(logratio))

word_ratios %>% 
    arrange(abs(logratio))

word_ratios

# TODO: print properly by author (word_ratios is by author)


word_ratios %>%
    mutate(abslogratio = abs(logratio)) %>%
    group_by(logratio < 0) %>%
    top_n(15, abslogratio) %>%
    ungroup() %>%
    mutate(word = reorder(word2, logratio)) %>%
    ggplot(aes(word, logratio, color = logratio < 0)) +
    geom_segment(aes(x = word, xend = word,
                     y = 0, yend = logratio), 
                 size = 1.1, alpha = 0.6) +
    geom_point(size = 3.5) +
    coord_flip() +
    labs(x = NULL, 
         y = "Relative appearance after 'she' compared to 'he'",
         title = "Words paired with 'he' and 'she' in HPL") +
    scale_color_discrete(name = "", labels = c("More 'she'", "More 'he'")) +
    scale_y_continuous(breaks = seq(-3, 3),
                       labels = c("0.125x", "0.25x", "0.5x", 
                                  "Same", "2x", "4x", "8x"))

```
```{r}
bigram_counts <- spooky_bigrams[spooky_bigrams$author == 'EAP',] %>%
    count(bigram, sort = TRUE) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(word1 %in% pronouns) %>%
    count(word1, word2, wt = n, sort = TRUE) %>%
    rename(total = nn)

bigram_counts

word_ratios <- bigram_counts %>%
    group_by(word2) %>%
    filter(sum(total) > 10) %>%
    ungroup() %>%
    spread(word1, total, fill = 0) %>%
    mutate_if(is.numeric, funs((. + 1) / sum(. + 1))) %>%
    mutate(logratio = log2(she / he)) %>%
    arrange(desc(logratio))

word_ratios %>% 
    arrange(abs(logratio))

word_ratios %>%
    mutate(abslogratio = abs(logratio)) %>%
    group_by(logratio < 0) %>%
    top_n(15, abslogratio) %>%
    ungroup() %>%
    mutate(word = reorder(word2, logratio)) %>%
    ggplot(aes(word, logratio, color = logratio < 0)) +
    geom_segment(aes(x = word, xend = word,
                     y = 0, yend = logratio), 
                 size = 1.1, alpha = 0.6) +
    geom_point(size = 3.5) +
    coord_flip() +
    labs(x = NULL, 
         y = "Relative appearance after 'she' compared to 'he'",
         title = "Words paired with 'he' and 'she' in EAP") +
    scale_color_discrete(name = "", labels = c("More 'she'", "More 'he'")) +
    scale_y_continuous(breaks = seq(-3, 3),
                       labels = c("0.125x", "0.25x", "0.5x", 
                                  "Same", "2x", "4x", "8x"))

```

# TF-IDF with lemmas

Doesn't change anything



# Readability

There exist several indexes to measure how "readable" sentences are, which compare the number of syllables of the words used and the length of the sentence. These indexes estimate the years of formal education a person needs to understand the text on the first reading.

The fog index is commonly used to confirm that text can be read easily by the intended audience. Texts for a wide audience generally need a fog index less than 12. Texts requiring near-universal understanding generally need an index less than 8.

Gunning Fog: 12 = High school senior | 14 = College sophomore | 17 = College graduate

```{r}
# with(spooky[spooky$author == 'EAP',], readability(text, NULL)) # 11.1 score | 14.6 Gunning Fog
# with(spooky[spooky$author == 'MWS',], readability(text, NULL)) # 11.5 score | 14.7 Gunning Fog
# with(spooky[spooky$author == 'HPL',], readability(text, NULL)) # 11.5 score | 14.6 Gunning Fog

x <- with(spooky, readability(text, author))
plot(x)
```

On average, the authors are almost as readable accross all sentences. let's look at readability per sentence.

```{r}
x2 <- with(spooky, readability(text, list(author, id)))
x2
```
```{r}
ggplot(x2, aes(x = Gunning_Fog_Index, fill = author)) +
      geom_histogram(binwidth=1) +
      xlim(c(0, 40)) + # there are outliers
      theme(legend.position = "none")
```


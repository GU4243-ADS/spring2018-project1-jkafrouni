---
title: "SPOOKY data experiments"
author: "Jerome Kafrouni"
date: "January 31, 2018"
output:
  html_document: default
  pdf_document: default
---

# Introduction

## Setup the libraries

```{r, message = F, warning = F}
packages.used <- c("ggplot2", "dplyr", "tibble", "tidyr",  "stringr", "tidytext", "topicmodels", "wordcloud", "ggridges")

# check packages that need to be installed.
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))

# install additional packages
if(length(packages.needed) > 0) {
  install.packages(packages.needed, dependencies = TRUE, repos = 'http://cran.us.r-project.org')
}

library(ggplot2)
library(dplyr)
library(tibble)
library(tidyr)
library(stringr)
library(tidytext)
library(topicmodels)
library(wordcloud)
library(ggridges)

source("../lib/multiplot.R")
```

## Read in the data
The following code assumes that the dataset `spooky.csv` lives in a `data` folder (and that we are inside a `docs` folder).

```{r}
spooky <- read.csv('../data/spooky.csv', as.is = TRUE)
```

## An overview of the data structure and content

Let's first remind ourselves of the structure of the data.
```{r}
head(spooky)
summary(spooky)
```

```{r}
# sum(is.na(spooky))
# spooky$author <- as.factor(spooky$author)
```

## Data Cleaning

```{r}
# Make a table with one word per row and remove `stop words` (i.e. the common words).
spooky_wrd <- unnest_tokens(spooky, word, text)
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")
```

## Word Frequency

Now we study some of the most common words in the entire data set.  With the below code we plot the fifty most common words in the entire datset. We see that "time", "life", and "night" all appear frequently.

```{r}
# Words is a list of words, and freqs their frequencies
words <- count(group_by(spooky_wrd, word))$word
freqs <- count(group_by(spooky_wrd, word))$n

head(sort(freqs, decreasing = TRUE))
# wordcloud(words, freqs, max.words = 50, color = c("purple4", "red4", "black"))
```


```{r}
# Counts number of times each author used each word.
author_words <- count(group_by(spooky_wrd, word, author))

# Counts number of times each word was used.
all_words    <- rename(count(group_by(spooky_wrd, word)), all = n)

author_words <- left_join(author_words, all_words, by = "word")
author_words <- arrange(author_words, desc(all))
author_words <- ungroup(head(author_words, 81))
  
ggplot(author_words) +
  geom_col(aes(reorder(word, all, FUN = min), n, fill = author)) +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~ author) +
  theme(legend.position = "none")
```

## Recall: number of sentences per author:

```{r}
ggplot(spooky) +
  geom_bar(aes(author, fill = author)) +
  theme(legend.position = "none")
```

## Punctuation

```{r}
spooky <- cbind(spooky, commas = mapply(function(x) str_count(x, ","), spooky$text))
spooky <- cbind(spooky, semi_columns = mapply(function(x) str_count(x, ";"), spooky$text))
spooky <- cbind(spooky, question = mapply(function(x) str_count(x, "\\?") > 0, spooky$text))
spooky <- cbind(spooky, exclamation = mapply(function(x) str_count(x, "\\!") > 0, spooky$text))
aggregate(commas ~ author, spooky, mean)
aggregate(semi_columns ~ author, spooky, mean)
aggregate(question ~ author, spooky, sum)
aggregate(exclamation ~ author, spooky, sum)

p1 <- ggplot(spooky, aes(x = commas, y = ..ncount.., fill = author)) + # use ..ncount.. to normalize the histogram (difft numbers of sentences per author)
      xlim(c(0, 15)) +
      geom_histogram(binwidth=1) +
      theme(legend.position = "none")

p2 <- ggplot(spooky, aes(x = semi_columns, y = ..ncount.., fill = author)) + # use ..ncount.. to normalize the histogram (difft numbers of sentences per author)
      xlim(c(0, 10)) +
      ylim(c(0,1)) +
      geom_histogram(binwidth=1) +
      theme(legend.position = "none")

p3 <- ggplot(aggregate(question ~ author, spooky, sum), aes(x = author, y = question, fill = author)) +
      geom_bar(stat="identity") +
      theme(legend.position = "none")
      

p4 <- ggplot(aggregate(exclamation ~ author, spooky, sum), aes(x = author, y = exclamation, fill = author)) +
      geom_bar(stat="identity") +
      theme(legend.position = "none")


layout <- matrix(c(1, 2, 1, 3), 2, 2, byrow = TRUE)
multiplot(p1, p2, p3, layout = layout)
```

## Dialogue

```{r}
spooky <- cbind(spooky, dialogue = mapply(function(x) str_count(x, '\\"') > 1, spooky$text))
spooky[spooky$dialogue == TRUE,]
# pb: some sentences have a " in the beginning but not at the end ??
aggregate(dialogue ~ author, spooky, sum)
# ATTENTION A NORMALISER LES SOMMES !
```

## Names in sentences

```{r}
# TODO (but maybe in POS)
# Number of uppercase except first character of the sentence:
spooky <- cbind(spooky, uppercase = mapply(function(x) str_count(x, ".[A-Z]"), spooky$text))
aggregate(uppercase ~ author, spooky, mean)
summary(spooky[spooky$author == 'EAP',])
# spooky[spooky$uppercase == 45,][1, 'text'] # outlier (text was not split properly)

hist(spooky[spooky$author == 'EAP',][, c('uppercase')])
boxplot(spooky[spooky$author == 'EAP',][, c('uppercase')])
```


# Part of Speech

```{r}
# install.packages("openNLP")
# install.packages("openNLPmodels.en", repos="http://datacube.wu.ac.at/" , type="source")
# install.packages("NLP")
library(rJava)
library(NLP)
library(openNLP)
library(openNLPmodels.en)

# NLP and ggplot2 both have a function named "annotate" which can cause a problem
# loading NLP after ggplot seems to fix the pb ??
```

```{r}
## Some text.
s <- "Pierre Vinken, and Jerome Kafrounisu 61 years old, will join the board"
s <- as.String(s) # nÃ©cessite library(NLP) ?
## Need sentence and word token annotations.
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
# a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))

entity_annotator <- Maxent_Entity_Annotator(kind="person")

# annotate(s, entity_annotator, a2)

contains_entity <- function(s) {
  a2 <- annotate(s, list(sent_token_annotator, word_token_annotator)) # needed ?
  a3 <- annotate(s, entity_annotator, a2)
  length(subset(a3, type == "entity")) > 0
}

# contains_entity("Hello I'm Jerome Kafrouni")
# mapply(contains_entity, head(spooky)$text)

spooky <- cbind(spooky, has_entity = mapply(contains_entity, spooky$text))

spooky[spooky$has_entity == TRUE,]
aggregate(has_entity ~ author, spooky, mean)


# spooky[sample(nrow(spooky), 3), ] # sampling 3 rows

# Full POS annotator:
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
y1 <- annotate(spooky[1, 'text'], list(sent_token_annotator, word_token_annotator))
y2 <- annotate(spooky[1, 'text'], pos_tag_annotator, y1)
y2
```


# Use of french words:

```{r}
# TODO
```

# Diversity of the vocabulary:

